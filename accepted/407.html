<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Preliminary Program</TITLE>

<style>

BODY {
   MARGIN-TOP: 15pt;
   MARGIN-LEFT: 15pt;
   MARGIN-RIGHT: 15pt;
   MARGIN-BOTTOM: 15pt;
   FONT-SIZE: 10pt;
   FONT-FAMILY: "Times New Roman";
   BACKGROUND-COLOR: #ffffff;
   COLOR: #000000;
}

P {
   FONT-SIZE: 10pt;
}

TD {
   FONT-SIZE: 10pt;
}

TH {
   FONT-SIZE: 10pt;
}


A {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:visited {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:active {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: none
}
A:hover {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: underline
}
H4 {
   FONT-SIZE: 10pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H3 {
   FONT-SIZE: 11pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H2 {
   FONT-SIZE: 12pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H1 {
   FONT-SIZE: 14pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

</style>

<p>

<h4>Optimizing Multivariate Performance Measures for Learning Relation Extraction Models</h4>

<em>Gholamreza Haffari<sup>1</sup>,&nbsp;Ajay Nagesh<sup>2</sup>,&nbsp;Ganesh Ramakrishnan<sup>3</sup></em><br>
<sup>1</sup>, <sup>2</sup>IIT Bombay, <sup>3</sup>Department of Computer Science and Engineering, Indian Institute of Technology Bombay

<p>

<hr>


<h4>Abstract</h4>

<blockquote>
    <p>We describe a novel max-margin learning approach to optimize non-linear performance measures for  distantly-supervised relation extraction models. Our approach can be generally used to learn latent variable models under multivariate non-linear performance measures, such as F_Î²-score. Our approach interleaves Concave-Convex Procedure &#40;CCCP&#41; for populating  latent variables with dual decomposition to factorize the original hard problem into smaller independent sub-problems. The experimental results demonstrate that our learning algorithm is more effective than the ones commonly used in the literature for distant supervision of information extraction models. On several data conditions, we show that our method outperforms the baseline and results in up to 8.5% improvement in the F_1-score. </p>
</blockquote>




<hr>

<p>
</body>
</html>