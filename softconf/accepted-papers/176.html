<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Preliminary Program</TITLE>

<style>

BODY {
   MARGIN-TOP: 15pt;
   MARGIN-LEFT: 15pt;
   MARGIN-RIGHT: 15pt;
   MARGIN-BOTTOM: 15pt;
   FONT-SIZE: 10pt;
   FONT-FAMILY: "Times New Roman";
   BACKGROUND-COLOR: #ffffff;
   COLOR: #000000;
}

P {
   FONT-SIZE: 10pt;
}

TD {
   FONT-SIZE: 10pt;
}

TH {
   FONT-SIZE: 10pt;
}


A {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:visited {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:active {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: none
}
A:hover {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: underline
}
H4 {
   FONT-SIZE: 10pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H3 {
   FONT-SIZE: 11pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H2 {
   FONT-SIZE: 12pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H1 {
   FONT-SIZE: 14pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

</style>

<p>

<h4>Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval</h4>

<em>Xiaodong Liu<sup>1</sup>,&nbsp;Jianfeng Gao<sup>2</sup>,&nbsp;Xiaodong He<sup>3</sup>,&nbsp;Li Deng<sup>3</sup>,&nbsp;Kevin Duh<sup>1</sup>,&nbsp;Ye-Yi Wang<sup>4</sup></em><br>
<sup>1</sup>Nara Institute of Science and Technology, <sup>2</sup>Microsoft Research, Redmond, <sup>3</sup>Microsoft Research, <sup>4</sup>Microsoft

<p>

<hr>


<h4>Abstract</h4>

<blockquote>
    <p>Methods of deep neural networks &#40;DNNs&#41; have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or single- task supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification &#40;for query classification&#41; and information retrieval &#40;ranking for web search&#41;, and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation and other multi-task learning experiments. </p>
</blockquote>




<hr>

<p>
</body>
</html>