<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Preliminary Program</TITLE>

<style>

BODY {
   MARGIN-TOP: 15pt;
   MARGIN-LEFT: 15pt;
   MARGIN-RIGHT: 15pt;
   MARGIN-BOTTOM: 15pt;
   FONT-SIZE: 10pt;
   FONT-FAMILY: "Times New Roman";
   BACKGROUND-COLOR: #ffffff;
   COLOR: #000000;
}

P {
   FONT-SIZE: 10pt;
}

TD {
   FONT-SIZE: 10pt;
}

TH {
   FONT-SIZE: 10pt;
}


A {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:visited {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}
A:active {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: none
}
A:hover {
   COLOR: #32426c;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
   TEXT-DECORATION: underline
}
H4 {
   FONT-SIZE: 10pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H3 {
   FONT-SIZE: 11pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H2 {
   FONT-SIZE: 12pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

H1 {
   FONT-SIZE: 14pt;
   FONT-FAMILY: Arial, Helvetica, sans-serif;
}

</style>

<p>

<h4>Accurate Evaluation of Segment-level Machine Translation Metrics</h4>

<em>Yvette Graham<sup>1</sup>,&nbsp;Timothy Baldwin<sup>2</sup>,&nbsp;Nitika Mathur<sup>2</sup></em><br>
<sup>1</sup>Trinity College Dublin, <sup>2</sup>The University of Melbourne

<p>

<hr>


<h4>Abstract</h4>

<blockquote>
    <p>Evaluation of segment-level machine translation metrics is currently hampered by: &#40;1&#41; low inter-annotator agreement levels in human assessments; &#40;2&#41; lack of an effective mechanism for evaluation of translations of equal quality; and &#40;3&#41; lack of methods of significance testing improvements over a baseline.  In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics.  We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed.  Three segment-level metrics --- Meteor, nLepor and sentBLEU-moses --- are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish to English and the aggregated set of 9 language pairs. </p>
</blockquote>




<hr>

<p>
</body>
</html>